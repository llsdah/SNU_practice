{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1aC0BmgMAkN7--_7ZgYpMT6WSkRwYh9QT"},"id":"57qXujNStABz","outputId":"a724005b-5e21-4d9e-9cc6-f9ba686e2113"},"outputs":[],"source":["## Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\n","import os\n","import sys\n","import platform\n","import pandas as pd\n","import re\n","from datetime import datetime, timedelta\n","\n","import requests\n","from bs4 import BeautifulSoup\n","#import html2text\n","from datetime import datetime\n","\n","\n","\n","def get_minutes_list(from_date='20000101'): # 기존 20050101 에서 20000101변경 \n","    prefix_addr = \"https://www.bok.or.kr\"\n","    from_date = datetime.strptime(from_date, '%Y%m%d')\n","    temp =0\n","    for pageIndex in range(1, 2): ## 46 페이지 == 2000,1,13, 49페이지 까지 있다. 전체 대상 \n","        url = 'https://www.bok.or.kr/portal/bbs/B0000245/list.do?menuNo=200761\u0026pageIndex={}'.format(pageIndex)\n","        user_agent = 'Mozilla/5.0'\n","        headers = {'User-Agent': user_agent}\n","        page = requests.get(url, headers=headers)\n","\n","        try:\n","            soup = BeautifulSoup(page.content, 'html.parser')\n","            brdList = soup.find_all('span', class_='col m10 s10 x9 ctBx')\n","            datainfo = soup.find_all('div', class_='col s12 dataInfo')\n","            print(\"brdList : \",brdList)\n","            print(\"datainfo : \",datainfo)\n","\n","            for post, data in zip(brdList, datainfo):\n","                guid = prefix_addr + post.a['href'][:-12] # -12 주의 머지?\n","                print(\"이동중입니다. : \",guid)\n","                temp+=1\n","                dl_file(guid,temp)\n","\n","\n","                # 여기서 부터 기존파일 보류건 현 파일에서는 필요없음\n","                \"\"\"\n","                desPage = requests.get(guid)\n","                desSoup = BeautifulSoup(desPage.content, 'html.parser')\n","                \n","\n","                title = post.find('span', class_='titlesub').get_text().strip()\n","\n","                mdate = title[title.find(')(') + 2:-1]\n","                if mdate[-1] == '.':\n","                    mdate = mdate[:-1]\n","                mdate = datetime.strptime(mdate, '%Y.%m.%d')\n","\n","                if mdate \u003c from_date: \n","                    break\n","\n","                rdate = data.find('span', class_='date').get_text().strip()\n","                rdate = datetime.strptime(rdate[3:], '%Y.%m.%d')\n","\n","                #get_minutes_file(guid, mdate, rdate)\n","                \"\"\"\n","\n","                \"\"\"\n","                description = desSoup.find('div', class_='dbData').get_text().strip() #공백 제거 \n","                print(\"description : \", description)\n","                if description.replace(' ', '').find('통화정책방향') \u003e= 0:\n","                    title = post.find('span', class_='titlesub').get_text().strip()\n","\n","                    mdate = title[title.find(')(') + 2:-1]\n","                    if mdate[-1] == '.':\n","                        mdate = mdate[:-1]\n","                    mdate = datetime.strptime(mdate, '%Y.%m.%d')\n","\n","                    if mdate \u003c from_date: # 잘 이해 안감. 끝날짜보다 작으면 그만둬라?\n","                        break\n","\n","                    rdate = data.find('span', class_='date').get_text().strip()\n","                    rdate = datetime.strptime(rdate[3:], '%Y.%m.%d')\n","\n","                    get_minutes_file(guid, mdate, rdate)\n","                \"\"\"\n","                \n","\n","\n","\n","        except:\n","            print(\"get url.content error and pass page{} it\".format(pageIndex))\n","\n","\n","\n","\n","\n","\n","#이거 기존 다운로드용 현재파일에서는 필요없음\n","def get_minutes_file(page_addr, mdate, rdate):\n","    file_header = 'data/minutes/pdf/KO_'\n","    prefix_addr = \"https://www.bok.or.kr\" # https\n","\n","    page = requests.get(page_addr)\n","    soup = BeautifulSoup(page.content, 'html.parser')\n","\n","    print(\"...\")\n","    try:\n","        links = soup.find('div', class_='addfile').find_all('a')\n","\n","        for link in links:\n","            filename = link.get_text()\n","            filename = filename.replace('\\r', '').replace('\\t', '').replace('\\n', '')\n","            \n","            ##실제 \u003ca href=\"/portal/cmmn/file/fileDown.do?menuNo=200761\u0026amp;atchFileId=FILE_000000000030802\u0026amp;fileSn=1\"\u003e\n","\t\t\t\t\t\t##\t\t\t금융통화위원회 의사록(2022년도 제7차)(2022.4.14).hwp\n","\t\t\t\t\t\t##\t\t\u003c/a\u003e\n","            ## http://bok.or.kr/portal/cmmn/file/fileDown.do?menuNo=200761\u0026amp;atchFileId=FILE_000000000030802\u0026amp;fileSn=1\n","            ## 현재 http://bok.or.kr/portal/cmmn/file/fileDown.do?menuNo=200761\u0026atchFileId=FILE_000000000028630\u0026fileSn=2\n","\n","            if filename[-3:] == 'pdf':\n","                filename = mdate.strftime('%Y%m%d') + \"_\" + rdate.strftime('%Y%m%d') + '.pdf'\n","                file_addr = prefix_addr + link[\"href\"]\n","                print( \"해당 주소로 이동중입니다. \",file_addr)\n","                file_res = requests.get(file_addr)\n","                filepath = file_header + filename\n","\n","                with open(filepath, 'wb') as f:\n","                    f.write(file_res.content)\n","\n","                print('save file name : ')\n","                print(filename)\n","    except:\n","        print(\"get file failed and pass it\")\n","\n","## usl 접속 후 다운로드 하지 \n","\n","#다운로드용 \n","def dl_file(url,temp) :\n","    print(\"url : \",url)\n","    user_agent = 'Mozilla/5.0'\n","    headers = {'User-Agent': user_agent}\n","    page = requests.get(url, headers=headers)\n","\n","    print(\"page.content : \",page.content)\n","    soup = BeautifulSoup(page.content, 'html.parser')\n","    file1 = soup.find('div', class_='addfile').find('ul').find('li').find('a').attrs['href']\n","    name = soup.find('h3', class_='subject').text\n","    # subject\n","    print(\"temp : \",temp,\" name : \",name)\n","\n","    link1 = 'http://www.bok.or.kr'+file1\n","\n","    file_res = requests.get(link1)\n","    \n","    with open('{}.pdf'.format(name),'wb') as f :\n","      f.write(file_res.content)\n","\n","\n","\n","get_minutes_list(from_date='20000101')\n","\n","\n","print( \"실행이 완료 되었습니당! \")\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pR_dSrhP1pah"},"source":["# 새 섹션"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUSbB_6IGZe7"},"outputs":[],"source":["## usl 접속 후 다운로드 하지 \n","\n","\n","def dl_file(url) :\n","    temp = 0\n","    user_agent = 'Mozilla/5.0'\n","    headers = {'User-Agent': user_agent}\n","    page = requests.get(url, headers=headers)\n","\n","    soup = BeautifulSoup(page.content, 'html.parser')\n","    file1 = soup.find_all('div', class_='addfile').find('a').attrs['href']\n","    \n","    print(\"temp : \",temp,\" datainfo : \",file1)\n","    temp = temp +1\n"]}],"metadata":{"accelerator":"GPU","anaconda-cloud":{},"colab":{"collapsed_sections":[],"name":"crawlingMinutes_수정(금통위의사록).ipynb","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"nbformat":4,"nbformat_minor":0}