{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "from pandas import read_table\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from ekonlpy.sentiment import MPCK\n",
        "\n",
        "mpck = MPCK()\n",
        "\n",
        "\n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, k=0.5):\n",
        "        self.k = k\n",
        "        self.word_probs = []\n",
        "\n",
        "    def load_corpusData(self, path):\n",
        "        corpusData = read_table(path, sep=',', header=None, names=None, encoding='utf-8')\n",
        "        corpusData = np.array(corpusData)\n",
        "\n",
        "        return corpusData\n",
        "\n",
        "    def count_words(self, training_set):\n",
        "        counts = defaultdict(lambda: [0, 0])\n",
        "\n",
        "        news_list = os.listdir('data/news/')\n",
        "        bonds_list = os.listdir('data/bonds/')\n",
        "        minutes_list = os.listdir('data/minutes/txt/')\n",
        "        file_list = news_list + bonds_list + minutes_list\n",
        "\n",
        "        for dataDate, label in training_set:\n",
        "            dataDate_without_dot = dataDate.replace('.', '')\n",
        "\n",
        "            for file in file_list:\n",
        "                if dataDate == file[5:15]:\n",
        "                    corpus = open('data/news/' + file, 'r', encoding='utf-8').read()\n",
        "                    print(\"process news file name : \", file)\n",
        "\n",
        "                    tokens = mpck.tokenize(corpus)\n",
        "                    ngrams = mpck.ngramize(tokens)\n",
        "\n",
        "                    for ngram in ngrams + tokens:\n",
        "                        counts[ngram][0 if label == 1 else 1] += 1\n",
        "                    print(\"complete\")\n",
        "\n",
        "                if dataDate == file[6:16]:\n",
        "                    corpus = open('data/bonds/' + file, 'r', encoding='utf-8').read()\n",
        "                    print(\"process bonds file name : \", file)\n",
        "\n",
        "                    tokens = mpck.tokenize(corpus)\n",
        "                    ngrams = mpck.ngramize(tokens)\n",
        "\n",
        "                    for ngram in ngrams + tokens:\n",
        "                        counts[ngram][0 if label == 1 else 1] += 1\n",
        "                    print(\"complete\")\n",
        "\n",
        "                if dataDate_without_dot == file[3:11]:\n",
        "                    corpus = open('data/minutes/txt/' + file, 'r', encoding='utf-8').read()\n",
        "                    print(\"process minutes file name : \", file)\n",
        "\n",
        "                    tokens = mpck.tokenize(corpus)\n",
        "                    ngrams = mpck.ngramize(tokens)\n",
        "\n",
        "                    for ngram in ngrams + tokens:\n",
        "                        counts[ngram][0 if label == 1 else 1] += 1\n",
        "                    print(\"complete\")\n",
        "\n",
        "        return counts\n",
        "\n",
        "    def word_probabilities(self, counts, total_class0, total_class1, k):\n",
        "        return [(w, (class0 + k) / (total_class0 + 2 * k), (class1 + k) / (total_class1 + 2 * k))\n",
        "                for w, (class0, class1) in counts.items()]\n",
        "\n",
        "    def train(self, trainfile_path):\n",
        "        training_set = self.load_corpusData(trainfile_path)\n",
        "\n",
        "        positive = len([1 for _, label in training_set if label == 1])\n",
        "        negative = len(training_set) - positive\n",
        "\n",
        "        word_counts = self.count_words(training_set)\n",
        "\n",
        "        self.word_probs = self.word_probabilities(word_counts, positive, negative, self.k)\n",
        "\n",
        "        for noOfWord in range(len(self.word_probs)):\n",
        "            if self.word_probs[noOfWord][1] / self.word_probs[noOfWord][2] > 1:\n",
        "                with open('data/res/positive.csv', 'a', encoding='utf-8') as f:\n",
        "                    f.write(self.word_probs[noOfWord][0] + '\\n')\n",
        "            else:\n",
        "                with open('data/res/negative.csv', 'a', encoding='utf-8') as f:\n",
        "                    f.write(self.word_probs[noOfWord][0] + '\\n')\n",
        "\n",
        "\n",
        "model = NaiveBayesClassifier()\n",
        "model.train(trainfile_path='data/labeledCallRate.csv')\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}