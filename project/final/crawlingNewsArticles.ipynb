{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- encoding: utf-8 -*-\n",
        "\n",
        "# News crawling from Naver\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "# from requests.adapters import HTTPAdapter\n",
        "# from urllib3.util.retry import Retry\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "from time import sleep\n",
        "import calendar\n",
        "import os\n",
        "\n",
        "\n",
        "# \uc5f0\ub3c4, \ub2ec\uc774 \uc8fc\uc5b4\uc9c0\uba74 \uadf8 \ub2ec\uc758 \ub9c8\uc9c0\ub9c9 \ub0a0 \ubc18\ud658\n",
        "def getLastDayOfMonth(year, month):\n",
        "    year = int(year)\n",
        "    month = int(month)\n",
        "    res = calendar.monthrange(year, month)\n",
        "    return str(res[1])\n",
        "\n",
        "\n",
        "search_start_year = '2007'  # \uc218\uc9d1 \uc2dc\uc791 \uc5f0\ub3c4\n",
        "search_start_month = '01'  # \uc218\uc9d1 \uc2dc\uc791 \ub2ec\n",
        "search_start_day = '01'  # \uc218\uc9d1 \uc2dc\uc791 \uc77c\uc790\n",
        "\n",
        "search_end_year = '2007'  # \uc218\uc9d1 \ub05d \uc5f0\ub3c4\n",
        "search_end_month = '01'  # \uc218\uc9d1 \ub05d \ub2ec\n",
        "search_end_day = getLastDayOfMonth(search_end_year, search_end_month)\n",
        "start_date = search_start_year + '.' + search_start_month + '.' + search_start_day  # 2005.01.01\n",
        "end_date = search_end_year + '.' + search_end_month + '.' + search_end_day  # 2017.12.31\n",
        "\n",
        "start_date_str = start_date.replace('.', '')\n",
        "end_date_str = end_date.replace('.', '')\n",
        "\n",
        "\n",
        "# param \ud615\uc2dd: '2005.01.01'\n",
        "# return: ['2005.01.01','2005.01.02' ... ]\n",
        "def getDatesPeriod(start, end):\n",
        "    from datetime import date, timedelta\n",
        "\n",
        "    start_str = start.split('.')\n",
        "    start_year = int(start_str[0])\n",
        "    start_month = int(start_str[1])\n",
        "    start_day = int(start_str[2])\n",
        "\n",
        "    end_str = end.split('.')\n",
        "    end_year = int(end_str[0])\n",
        "    end_month = int(end_str[1])\n",
        "    end_day = int(end_str[2])\n",
        "    d1 = date(start_year, start_month, start_day)  # start date\n",
        "    d2 = date(end_year, end_month, end_day)  # end date\n",
        "\n",
        "    delta = d2 - d1  # timedelta\n",
        "\n",
        "    for i in range(delta.days + 1):\n",
        "        date = str(d1 + timedelta(i))\n",
        "        date = date.replace('-', '.')\n",
        "\n",
        "    res = [str(d1 + timedelta(i)).replace('-', '.') for i in range(delta.days + 1)]\n",
        "    return res\n",
        "\n",
        "\n",
        "def getLastPageNum(soup):\n",
        "    # #main_pack > div.news.mynews.section._prs_nws > div.section_head > div.title_desc.all_my > span\n",
        "    # '\uae08\ub9ac' \uac80\uc0c9\uacb0\uacfc \ucd1d \uae30\uc0ac\uac2f\uc218 \uc815\ubcf4 \uac00\uc838\uc624\uae30\n",
        "    try:\n",
        "        tmp = soup.select_one(\n",
        "            '#main_pack > div.news.mynews.section._prs_nws > div.section_head > div.title_desc.all_my').text  # 1-10 / 76,863\uac74\n",
        "\n",
        "    except AttributeError:\n",
        "        print('\ud574\ub2f9 \uc77c\uc790 \"\uae08\ub9ac\" \uac80\uc0c9\uacb0\uacfc \uc5c6\uc74c')\n",
        "        return 0\n",
        "\n",
        "    tmp = tmp.split('/ ')  # 1-10, 76,863\uac74\n",
        "    tmp = tmp[1].split('\uac74')  # 76,863\n",
        "    tmp = tmp[0].replace(',', '')\n",
        "    total_article_num = int(tmp)\n",
        "    print('\uac80\uc0c9\uacb0\uacfc \ucd1d \uae30\uc0ac\uac2f\uc218[\uc2e0\ubb38\uc0ac 3\uac1c \uc774\uc678\uc758 \ub2e4\ub978 \uc2e0\ubb38\uc0ac \uac80\uc0c9 \uacb0\uacfc\ub3c4 \uac19\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\uc74c \uc8fc\uc758]:', total_article_num, ' \uac1c')\n",
        "\n",
        "    last_page = int(total_article_num / 10)\n",
        "    if total_article_num % 10 > 0:\n",
        "        last_page += 1\n",
        "    print('\uac80\uc0c9\uacb0\uacfc \ub05d\ud398\uc774\uc9c0:', last_page, ' \ud398\uc774\uc9c0')\n",
        "    return last_page\n",
        "\n",
        "\n",
        "#          \uc77c\uc790   \uc77c\uc790  1->11->21.. \ub2e4\uc74c\ud398\uc774\uc9c0 \uc870\ud68c\n",
        "def getURL(start, end, first_article_id):\n",
        "    start_string = start.replace('.', '')\n",
        "    end_string = end.replace('.', '')\n",
        "    # url (\ub124\uc774\ubc84 \ub274\uc2a4 \uc870\uac74\uac80\uc0c9 - 2\ud398\uc774\uc9c0)\n",
        "    # https://search.naver.com/search.naver?&where=news&query=%EA%B8%88%EB%A6%AC&sm=tab_pge&sort=2&photo=0&field=0&reporter_article=&pd=3&ds=2005.01.01&de=2017.12.31&docid=&nso=so:da,p:from20050101to20171231,a:all&mynews=1&start=11&refresh_start=0\n",
        "    url = 'https://search.naver.com/search.naver?' \\\n",
        "          '&where=news&query=%EA%B8%88%EB%A6%AC' \\\n",
        "          '&sm=tab_pge&sort=2&photo=0&field=0' \\\n",
        "          '&reporter_article=&pd=3&ds=' + start + '&de=' + end + \\\n",
        "          '&docid=&nso=so:da,p:from' + start_string + 'to' + end_string + ',a:all' \\\n",
        "                                                                          '&mynews=1&start=' + first_article_id + '&refresh_start=0'\n",
        "    return url\n",
        "\n",
        "\n",
        "# \uae30\uc0ac url \uc5f4\uc5b4\uc11c \uae30\uc0ac \ub0b4\uc6a9 \ubc18\ud658\n",
        "def getContent(url, headers):\n",
        "    # session = requests.Session()\n",
        "    # retry = Retry(connect=3, backoff_factor=0.5)\n",
        "    # adapter = HTTPAdapter(max_retries=retry)\n",
        "    # session.mount('http://', adapter)\n",
        "    # session.mount('https://', adapter)\n",
        "    #\n",
        "    # session.headers.update(headers)\n",
        "    # session.get(url)\n",
        "\n",
        "    sleep(random.uniform(1.0, 1.5))\n",
        "\n",
        "    res = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "    try:\n",
        "        text = soup.select_one('#articleBodyContents').text\n",
        "        article_content = text.split('back() {}')[1]\n",
        "    except AttributeError:\n",
        "        # print('AttributeError')\n",
        "        article_content = None\n",
        "\n",
        "    return article_content\n",
        "\n",
        "\n",
        "# \ub124\uc774\ubc84\uac00 \uc544\ub2cc \uac01 \uc2e0\ubb38\uc0ac \ud648\ud398\uc774\uc9c0\uc5d0\uc11c raw content \uac00\uc9c0\uace0 \uc624\uae30\n",
        "def getContentFromOtherSite(url, headers, source):\n",
        "    sleep(random.uniform(1.0, 1.5))\n",
        "\n",
        "    res = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "\n",
        "    if source == '\uc5f0\ud569\uc778\ud3ec\ub9e5\uc2a4':\n",
        "        try:\n",
        "            text = soup.select_one('#article-view-content-div').text\n",
        "            article_content = text\n",
        "        except AttributeError:\n",
        "            # print('\uae30\uc0ac\uc0ad\uc81c\ub428')\n",
        "            article_content = None\n",
        "    elif source == '\uc5f0\ud569\ub274\uc2a4':\n",
        "        try:\n",
        "            # #articleWrap > div.article\n",
        "            text = soup.select_one('#articleWrap > div.article').text\n",
        "\n",
        "            article_content = text\n",
        "        except AttributeError:\n",
        "            # print('\uae30\uc0ac\uc0ad\uc81c\ub428')\n",
        "            article_content = None\n",
        "\n",
        "    else:\n",
        "        try:\n",
        "            # #contents > section.center1080.position_r > section.aside_left > div.article_news > div.newscontainer > div.news_body\n",
        "            text = soup.select_one('div.newscontainer > div.news_body').text\n",
        "            article_content = text\n",
        "        except AttributeError:\n",
        "            # print('\uae30\uc0ac\uc0ad\uc81c\ub428')\n",
        "            article_content = None\n",
        "    return article_content\n",
        "\n",
        "\n",
        "# \uae30\uc0ac \ub0b4\uc6a9\uc911 header/footer\ubd80\ubd84(\uae30\uc790\uba85,\uae30\uc0ac\ucd9c\ucc98\uba85,(\ub05d),\uc800\uc791\uad8c\uba85\uba85)\uc744 \uc81c\uac70\ud558\uace0\n",
        "# \uae30\uc0ac \ub0b4\uc6a9 \ubc18\ud658\n",
        "def removeHeaderFooter(type, text):\n",
        "    # \uae30\uc0ac \ucd9c\ucc98\uc5d0 \ub530\ub77c \uc801\uc6a9\ud558\ub294 \uc815\uaddc\uc2dd\uc774 \ub2e4\ub984\n",
        "    if type in ['\uc5f0\ud569\ub274\uc2a4', '\uc5f0\ud569\uc778\ud3ec\ub9e5\uc2a4']:\n",
        "        # header \uc81c\uac70\n",
        "        p_yhnews = re.compile('\\(.+?\uc5f0\ud569\ub274\uc2a4')  # \ucd9c\ucc98\ud45c\uc2dc\uac80\uc0c9 ex.(\uc11c\uc6b8=\uc5f0\ud569\ub274\uc2a4)\n",
        "        res = p_yhnews.findall(text)\n",
        "        # (\uc11c\uc6b8=\uc5f0\ud569\ub274\uc2a4) \uc788\uc73c\uba74 \uc81c\uac70\n",
        "        if res:\n",
        "            text = text.replace(res[0], '')\n",
        "\n",
        "        p_yhinfo = re.compile('\\(.+?\uc5f0\ud569\uc778\ud3ec\ub9e5\uc2a4')\n",
        "        res = p_yhinfo.findall(text)\n",
        "        # (\uc11c\uc6b8=\uc5f0\ud569\uc778\ud3ec\ub9e5\uc2a4) \uc788\uc73c\uba74 \uc81c\uac70\n",
        "        if res:\n",
        "            text = text.replace(res[0], '')\n",
        "\n",
        "        p_reporter = re.compile('\\).+?\uae30\uc790\\s{0,1}=')  # \uae00\uc4f4\uc774 \uc815\ubcf4 \uac80\uc0c9 #\uae30\uc790\n",
        "        res = p_reporter.findall(text)\n",
        "        if res:\n",
        "            text = text.replace(res[0], '')\n",
        "\n",
        "        p_cor = re.compile('\\).+?\ud2b9\ud30c\uc6d0\\s{0,1}=')  # \ud2b9\ud30c\uc6d0\n",
        "        res = p_cor.findall(text)\n",
        "        if res:\n",
        "            text = text.replace(res[0], '')\n",
        "\n",
        "        # footer \uc81c\uac70\n",
        "        text = re.split(r'[(][\ub05d][)]', text)[0]  # footer \uc81c\uac70\n",
        "\n",
        "    else:\n",
        "        # type:\uc774\ub370\uc77c\ub9ac\n",
        "        # header \uc81c\uac70\n",
        "        p_edlyhd = re.compile('\\[.+?\\]')  # [\uc774\ub370\uc77c\ub9ac \uc548\ud61c\uc2e0 \uae30\uc790]\n",
        "        res = p_edlyhd.findall(text)\n",
        "        if res:\n",
        "            text = text.replace(res[0], '')\n",
        "\n",
        "        # footer \uc81c\uac70\n",
        "        # \uc720\ud615 1\n",
        "        p_edlyft = re.compile('<.+?>')\n",
        "        res = p_edlyft.findall(text)\n",
        "        if res:\n",
        "            text = text.replace(res[-1], '')\n",
        "        # \uc720\ud615 2 \uff1c\n",
        "        p_edlyft = re.compile('\uff1c.+?\uff1e')\n",
        "        res = p_edlyft.findall(text)\n",
        "        if res:\n",
        "            text = text.replace(res[-1], '')\n",
        "    return text\n",
        "\n",
        "\n",
        "def writeFirstlineOfContentOnFile(type, text, f):\n",
        "    # \uae30\uc0ac \ub0b4\uc6a9\uc911 \uccab\ubc88\uc9f8 \uc904\ub9cc \ubd84\ub9ac --> header \ud615\ud0dc \ubcf4\uae30\uc704\ud55c \ud568\uc218\n",
        "    res = re.split(r'[.]', text)[0]\n",
        "    data = '%s, %s\\n' % (type, res)\n",
        "    f.write(data)\n",
        "\n",
        "\n",
        "# import the os module\n",
        "import os\n",
        "\n",
        "# detect the current working directory and print it\n",
        "path = os.getcwd()\n",
        "path = path + \"/naverNews_output\"\n",
        "\n",
        "try:\n",
        "    if os.path.exists(path):\n",
        "        print('/naverNews_output \ud3f4\ub354 \uc774\ubbf8 \uc874\uc7ac')\n",
        "    else:\n",
        "        print('/naverNews_output \ud3f4\ub354 \uc0dd\uc131')\n",
        "        os.mkdir(path)\n",
        "except OSError:\n",
        "    print(\"\ud3f4\ub354 \uc0dd\uc131 \uc2e4\ud328 %s\" % path)\n",
        "else:\n",
        "    pass\n",
        "\n",
        "# output to file\n",
        "current_time = time.strftime(\"%m_%d_%H_%M\", time.localtime())\n",
        "\n",
        "dates_list = getDatesPeriod(start_date, end_date)\n",
        "article_cnt = 0  # \uae01\uc5b4\uc628 \ucd1d \uae30\uc0ac\uac2f\uc218\n",
        "\n",
        "print('\uac80\uc0c9 \uae30\uac04:', start_date, '~', end_date)\n",
        "for date in dates_list:\n",
        "    article_daily_cnt = 0  # \ud574\ub2f9 \uc77c\uc790 \uae01\uc5b4\uc628 \uae30\uc0ac \uac2f\uc218\n",
        "    start_article_id = '1'\n",
        "\n",
        "    output_name = 'naverNews_output/news_%s_%s.txt' % (date, current_time)\n",
        "    f = open(output_name, 'w', -1, \"utf-8\")\n",
        "    print('==')\n",
        "    print('\uac80\uc0c9 \uc77c\uc790: ', date)\n",
        "    print('\uacb0\uacfc \ud30c\uc77c: ', output_name)\n",
        "\n",
        "    last_page = 1\n",
        "    current_page = 1\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "\n",
        "    # '\uae08\ub9ac'\ub85c \ub124\uc774\ubc84 \ub274\uc2a4 \uae30\uc0ac \uac80\uc0c9\n",
        "    # \ud398\uc774\uc9c0\ubcc4\ub85c \uacb0\uacfc \ubc1b\uc544\uc624\uae30\n",
        "    while current_page <= last_page:\n",
        "        # \ud574\ub2f9\ud398\uc774\uc9c0\uc758 \ub274\uc2a4 10\uac1c url\uc744 \ubaa8\ub450 \uae01\uc5b4\uc628\ub2e4\n",
        "        sleep(random.uniform(1.0, 1.5))\n",
        "        url = getURL(date, date, start_article_id)\n",
        "        res = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(res.text, 'html.parser')\n",
        "\n",
        "        if current_page == 1:\n",
        "            print(url)\n",
        "            last_page = getLastPageNum(soup)\n",
        "            if last_page == 0:\n",
        "                break\n",
        "\n",
        "        # \uac80\uc0c9\uacb0\uacfc \ud398\uc774\uc9c0 \uc77d\uc5b4\uc624\uae30\n",
        "        # #main_pack > div.news.mynews.section._prs_nws > ul\n",
        "        # \ud558\uc704\uc5d0 <li> \ud0dc\uadf8\uac00 10\uac1c \uc788\uc74c\n",
        "        article_boxes = soup.select('#main_pack > div.news.mynews.section._prs_nws > ul > li > dl')\n",
        "        # sp_nws1 > dl > dt > a \uc81c\ubaa9 url\n",
        "        # #sp_nws1 > dl > dd.txt_inline > span._sp_each_source \uc2e0\ubb38\uc0ac\n",
        "        # sp_nws1 > dl > dd.txt_inline > span:nth-child(2)\n",
        "        # #sp_nws1 > dl > dd.txt_inline \uc758 text \uc694\uc18c \ub0a0\uc9dc\n",
        "\n",
        "        # num_of_articles_on_page = len(article_boxes)\n",
        "        for box in article_boxes:\n",
        "            flag_naverPageExist = True\n",
        "            try:\n",
        "                article_source = box.select_one('dd.txt_inline > span._sp_each_source').text\n",
        "            except AttributeError:\n",
        "                # \uc2e0\ubb38\uc0ac \ucd9c\ucc98 \uc5c6\ub294 \uacbd\uc6b0\n",
        "                print('\uc2e0\ubb38\uc0ac \ucd9c\ucc98 \uc5c6\uc74c')\n",
        "                continue\n",
        "\n",
        "            # \uae30\uc0ac \ucd9c\ucc98\uac00 \uc774\ub370\uc77c\ub9ac, \uc5f0\ud569\ub274\uc2a4, \uc5f0\ud569\uc778\ud3ec\ub9e5\uc2a4\uc778 \uacbd\uc6b0\ub9cc \uae30\uc0ac \ud06c\ub864\ub9c1\n",
        "            source_list = ['\uc774\ub370\uc77c\ub9ac', '\uc5f0\ud569\ub274\uc2a4', '\uc5f0\ud569\uc778\ud3ec\ub9e5\uc2a4']\n",
        "\n",
        "            if article_source in source_list:\n",
        "                try:\n",
        "                    article_url = box.select_one('dd.txt_inline > a').get(\"href\")\n",
        "                except AttributeError:\n",
        "                    # \uae30\uc0ac \uc0c1\uc138 \uc815\ubcf4(\ub124\uc774\ubc84 \ub274\uc2a4 \ub9c1\ud06c) \uc5c6\ub294 \uacbd\uc6b0\n",
        "                    flag_naverPageExist = False\n",
        "                    article_url = box.select_one('dt > a').get('href')\n",
        "\n",
        "                try:\n",
        "                    article_title = box.select_one('dt > a').get(\"title\")\n",
        "                    article_date = box.select_one('dd.txt_inline').text\n",
        "                except AttributeError:\n",
        "                    # \uae30\uc0ac \uc0c1\uc138 \uc815\ubcf4 \uc5c6\ub294 \uacbd\uc6b0\n",
        "                    print('\uae30\uc0ac \uc0c1\uc138 \uc815\ubcf4 \uc5c6\uc74c')\n",
        "                    continue\n",
        "\n",
        "                article_date = article_date.split(' ')[2]\n",
        "\n",
        "                if flag_naverPageExist:\n",
        "                    article_raw_content = getContent(article_url, headers)\n",
        "                else:\n",
        "                    article_raw_content = getContentFromOtherSite(article_url, headers, article_source)\n",
        "\n",
        "                if article_raw_content is None:\n",
        "                    # \uae30\uc0ac \uc0ad\uc81c\ub418\uc11c \ub9c1\ud06c \ubabb \uc5f4\uacbd\uc6b0 \ub2e4\ub978 \uae30\uc0ac\ub85c \ub118\uc5b4\uac10\n",
        "                    continue\n",
        "\n",
        "                # header, footer \uc81c\uac70\n",
        "                article_content = removeHeaderFooter(article_source, article_raw_content)\n",
        "                article_content = article_content.strip()\n",
        "                info = '%s@@@%s@@@%s@@@%s@@@%s' \\\n",
        "                       % (article_source,\n",
        "                          date,\n",
        "                          article_title,\n",
        "                          article_url,\n",
        "                          article_content)\n",
        "                # \ud06c\ub864\ub9c1\ud55c \uae30\uc0ac \ucd9c\ucc98, \ub0a0\uc9dc, \uc81c\ubaa9, \ub0b4\uc6a9\n",
        "                f.write(info)\n",
        "                f.write(article_content)\n",
        "                f.write('\\n===\\n')\n",
        "\n",
        "                #                 if article_cnt % 100 == 0:\n",
        "                #                     current_time = time.strftime(\"%m_%d_%H_%M\", time.localtime())\n",
        "                #                     print('\uc218\uc9d1 \uae30\uc0ac \uac2f\uc218: ',article_cnt,' \uae30\uc0ac \ub0a0\uc9dc: ', article_date, ' \uc218\uc9d1 \uc2dc\uac01: ',current_time)\n",
        "                article_cnt += 1\n",
        "                article_daily_cnt += 1\n",
        "\n",
        "        start_article_id = str(int(start_article_id) + 10)  # \ub2e4\uc74c\ud398\uc774\uc9c0\ub85c \ub118\uc5b4\uac04\ub2e4\n",
        "        current_page += 1\n",
        "    f.flush()\n",
        "    f.close()\n",
        "    # \uc77c\uc790\ubcc4\n",
        "    current_time = time.strftime(\"%m_%d_%H_%M\", time.localtime())\n",
        "    print('\ud574\ub2f9 \uc77c\uc790 \uc218\uc9d1 \uae30\uc0ac \uac2f\uc218: ', article_daily_cnt\n",
        "          , ' \uc77c\uc790: ', article_date\n",
        "          , ' \uc2dc\uac01(\uc11c\ubc84\uc2dc\uac04): ', current_time\n",
        "          , ' \ub204\uc801 \uc218\uc9d1 \uae30\uc0ac \uac2f\uc218: ', article_cnt)\n",
        "\n",
        "print('\uc218\uc9d1\ud55c \ucd1d \uae30\uc0ac \uac2f\uc218', article_cnt)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}